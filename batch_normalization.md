# Batch Normalization

# Abstract

前層のパラメーターによって各層への入力情報の分布が学習中に変化していくため、
深層学習モデルの学習は複雑になっている。
この現象を internal covariate shift（内部共変性シフト; ICS）と呼び、本論文では正規化によってこの問題を回避する手法を提案している。

# ICS について

学習中にネットワークのパラメーターが変化することで、ネットワークの活性化関数を通過したあとの分布が変わることを、internal covariate shift と定義している。先行研究では入力を白色化（平均0、標準偏差1、無相関化）することでICSを回避し、学習の収束スピードが速まるということが知られていた[1]。


# Normalization via Mini-Batchg Statistics

## 概要

各レイヤーの入力情報の白色化は計算コストが高いため、2つの単純化のための手法を用いる。

1. 特徴量ベクトルの各次元ごとに正規化する
2. ミニバッチ単位で処理を行う

$d$ 次元特徴量ベクトルからなる、サイズ $m$ のミニバッチのデータ $\mathcal{B} = \{\bm{x}_1,...,\bm{x}_m\}$ を考える。
Batch Normalization（以下BN）は特徴量ベクトルの各次元に対して独立に計算するので、$k$次元目の特徴量 $\{x_1^{(k)},...,x_m^{(k)}\}$ に対して以下の計算を行う（ただし簡単のために $(k)$　の部分は省略して表記する）。

- ミニバッチ内での平均、分散の計算
$$
\mu = \frac{1}{m}\sum x_i
$$
$$
\sigma^2 = \frac{1}{m}\sum (x_i-\mu)^2
$$

- 正規化
$$
\hat{x_i} = \frac{x_i - \mu}{\sqrt{\sigma^2+\epsilon}}
$$

- BNの変換（scale & shift）
$$
y_i = \gamma \hat{x_i} + \beta
$$

BNの出力としては $y_i$ であり、これが後段のレイヤーへの入力情報となる。
これにより学習中に極力ICSの影響を排除してパラメータの更新を行うことができ、学習スピードが向上することになる。

### scale & shift について

正規化を行ったときに $\gamma$ と $\beta$ のパラメータを導入しているが、これはネットワークの表現力を落とさないためのテクニックである。

$\hat{x_i}$のままだと例えばそれをシグモイド活性化関数に入力した際に、$E[x_i]=0,V[x_i]=1$であるので非線形領域の右端・左端の値を持ちにくく原点付近の値を持ってしまうため、線形的な表現力しか持たなくなってしまう。
そこで「そういう表現力がほしい」と学習過程でモデルが判断すれば、BN変換が恒等変換となるように $\gamma$ と $\beta$ が学習可能パラメータとして導入されている。例えば、$\gamma=\sqrt{V[x]}$、$\beta=E[x]$ となるように学習すれば $y_i = x_i$ となるので BN変換は恒等変換になる。


## Training and Inference

BN変換を $BN(x)$ で表すこととすると、正規化のための平均や分散はミニバッチのとり方によって学習中に毎回異なることになる。
推論時には決定的（deterministic）にしたいので、平均・分散をミニバッチの平均に置き換える。
$$
\hat{x} = \frac{x-E[x]}{\sqrt{V[x] + \epsilon}}
$$
を推論時には使用する。

ミニバッチでの（標本）平均は
$$
\mu_B = \frac{1}{m}\sum_i x_i
$$
であり、
$$
E[\mu_B] = E \left[\frac{1}{m}\sum_i x_i \right] = \mu
$$
不偏推定量である。また同様にミニバッチでの（標本）分散は
$$
\sigma_B^2 = \frac{1}{m}\sum_i (x_i - \mu_B)^2
$$
であり、
$$
E[\sigma_B^2] = E \left[\frac{1}{m}\sum_i (x_i - \mu_B)^2 \right] = ... = \frac{m-1}{m} \sigma^2
$$
となるため、不偏推定量とするには $m/(m-1)$ の調整が必要となる（標本分散の不偏性のいつもの議論と同じ）。
以上からミニバッチでの平均・分散の平均を取ることで、推論時の正規化に使用する平均 $E[x]$ と分散 $V[x]$ を計算する。

## Batch-normalized CNN

BN変換をどこに入れればいいかを考える。そこでFCNNやCNNを想定し、前レイヤーの出力値 $u$ を重み $W$ とバイアス $b$ で変換した値 $Wu + b$ を活性化関数に通す場合を考える。
$$
z = g(Wu + b)
$$
BNによる正規化は

- $u$ に対して行うパターン
- $Wu+b$ に対して行うパターン

を考える。前者は、$u$ は前レイヤーの非線形な出力値であり、学習中に $u$ の分布は変化しやすい。後者は 

> $Wu+b$ is more likely to have a symmetric, non-sparse distribution,...

とあり（よく分からず...）、$Wu+b$ に対して正規化を掛けたほうがよい、とうい議論が行われている。そのため、
$$
z = g(BN(Wu))
$$
という形でバッチノーマライゼーションが実行されるということが導入されている。


# 参考文献

- [1] A Convergence Analysis of Log-Linear Training and its Application to Speech Recognition